{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a09125",
   "metadata": {},
   "source": [
    "In this simple notebook we use the package `lm-scorer` to calculate the log probability of a sentence based on a GPT2 language model.\n",
    "\n",
    "The initial idea was to use this:\n",
    "\n",
    "https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175\n",
    "\n",
    "But it seemed complicated and not intuitive and I was lazy.\n",
    "\n",
    "The goal now is to fine-tune a GPT2 model with poetry to see whether a text \"sounds\" like poetry or not (fluency feature in Erato)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63df3650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'lm-scorer'...\n",
      "remote: Enumerating objects: 396, done.\u001b[K\n",
      "remote: Counting objects: 100% (114/114), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 396 (delta 106), reused 101 (delta 101), pack-reused 282\u001b[K\n",
      "Receiving objects: 100% (396/396), 4.68 MiB | 3.71 MiB/s, done.\n",
      "Resolving deltas: 100% (214/214), done.\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/simonepri/lm-scorer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4078fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mv lm-scorer/lm_scorer/ ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee4b8141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_scorer.models.auto import AutoLMScorer as LMScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d5ab9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.018321018666028976,\n",
       "  0.006643158383667469,\n",
       "  0.08063224703073502,\n",
       "  0.0006074536358937621,\n",
       "  0.27771326899528503,\n",
       "  0.003638095920905471],\n",
       " [40, 588, 428, 5301, 13, 50256],\n",
       " ['I', 'Ġlike', 'Ġthis', 'Ġpackage', '.', '<|endoftext|>'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from lm_scorer.models.auto import AutoLMScorer as LMScorer\n",
    "\n",
    "# Available models\n",
    "list(LMScorer.supported_model_names())\n",
    "# => [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", distilgpt2\"]\n",
    "\n",
    "# Load model to cpu or cuda\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 1\n",
    "scorer = LMScorer.from_pretrained(\"gpt2\", device=device, batch_size=batch_size)\n",
    "\n",
    "# Return token probabilities (provide log=True to return log probabilities)\n",
    "scorer.tokens_score(\"I like this package.\")\n",
    "# => (scores, ids, tokens)\n",
    "# scores = [0.018321, 0.0066431, 0.080633, 0.00060745, 0.27772, 0.0036381]\n",
    "# ids    = [40,       588,       428,      5301,       13,      50256]\n",
    "# tokens = [\"I\",      \"Ġlike\",   \"Ġthis\",  \"Ġpackage\", \".\",     \"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d70e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.023056185744391e-12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute sentence score as the product of tokens' probabilities\n",
    "scorer.sentence_score(\"I like this package.\", reduce=\"prod\")\n",
    "# => 6.0231e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "905130a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06459253281354904"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute sentence score as the mean of tokens' probabilities\n",
    "scorer.sentence_score(\"I like this package.\", reduce=\"mean\")\n",
    "# => 0.064593\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e947576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013488681055605412"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute sentence score as the geometric mean of tokens' probabilities\n",
    "scorer.sentence_score(\"I like this package.\", reduce=\"gmean\")\n",
    "# => 0.013489\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d5df030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002800857648253441"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute sentence score as the harmonic mean of tokens' probabilities\n",
    "scorer.sentence_score(\"I like this package.\", reduce=\"hmean\")\n",
    "# => 0.0028008\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b3d60ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25.835426330566406"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the log of the sentence score.\n",
    "scorer.sentence_score(\"I like this package.\", log=True)\n",
    "# => -25.835\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1529df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1507757420592402e-11, 5.66442205640616e-12]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score multiple sentences.\n",
    "scorer.sentence_score([\"Sentence 1\", \"Sentence 2\"])\n",
    "# => [1.1508e-11, 5.6645e-12]\n",
    "\n",
    "# NB: Computations are done in log space so they should be numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18563f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.696429967670056e-11, 8.797713635887421e-15]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score multiple sentences.\n",
    "scorer.sentence_score([\"This is bullshit\", \"This bullshit is\"])\n",
    "# => [1.1508e-11, 5.6645e-12]\n",
    "\n",
    "# NB: Computations are done in log space so they should be numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a8ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:superpytorch]",
   "language": "python",
   "name": "conda-env-superpytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
