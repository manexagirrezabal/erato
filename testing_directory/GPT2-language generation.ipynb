{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cec496f",
   "metadata": {},
   "source": [
    "Code from\n",
    "\n",
    "https://huggingface.co/gpt2\n",
    "\n",
    "Information about fine-tuning model:\n",
    "\n",
    "https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272\n",
    "\n",
    "Further reading about autoregressive generation:\n",
    "\n",
    "https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e634213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d7be987e8946b0b5ecb3fe2c386b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f5d405fd0648db87f0df478d1629c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c3a94f077b42deb05838375aa51741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e5cd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, and as such, I am here to introduce it to everyone. Languages, after all, are a means to\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and as such, I'm in no way an expert in writing such a model. For one thing, I\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and as such, I've created a new framework that works with Python's built-in type system. I\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and as such, I was never really satisfied with the way language models evolve after that.\\n\\nQ:\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and as such, I think that one can be taught and applied as well. (...) It's really not\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model, and as such, I\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3fbc0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I am an astronaut and I live in a family living in Atlanta, and I like people that want to understand the world, and to look at their'},\n",
       " {'generated_text': 'I am an astronaut and I live in a country with limited resources. Why do we have only 30 times the amount of energy per night you need to'},\n",
       " {'generated_text': 'I am an astronaut and I live in an apartment that has been built with the greatest effort and energy into the first week.\"\\n\\nI went on'},\n",
       " {'generated_text': 'I am an astronaut and I live in a small town in Hawaii. Because I am an astronaut, I usually get to stay in the United States and'},\n",
       " {'generated_text': 'I am an astronaut and I live in a world with space, and I am very curious as to what goes on if you happen to notice that the'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"I am an astronaut and I live in\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2027685e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"The capital of the country that I am living is Berlin. The country is called Berlin and I have lived there for 27 years now. I didn't\"},\n",
       " {'generated_text': 'The capital of the country that I am living is Berlin. The country is called Berlin, because it is now a bustling exporter of goods and raw'},\n",
       " {'generated_text': 'The capital of the country that I am living is Berlin. The country is called Berlin, as does Germany. Of course our city is a little tiny'},\n",
       " {'generated_text': 'The capital of the country that I am living is Berlin. The country is called GDR. Berlin is my home.\"\\n\\nWhen asked about the'},\n",
       " {'generated_text': 'The capital of the country that I am living is Berlin. The country is called Berlin without Berlin. The capital of Berlin is Berlin without Berlin. It'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The capital of the country that I am living is Berlin. The country is called\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3645c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'No more of talk where god or angel guests show her how to be happy.\\n\\n5. It looks as though the guy at the bar was'},\n",
       " {'generated_text': \"No more of talk where god or angel guests are seen, but rather get them the hell out of here for now. There's nothing much to worry\"},\n",
       " {'generated_text': 'No more of talk where god or angel guests stand around doing nothing.\" â€” A.D. 1292, \"The Children of Israel\" (K'},\n",
       " {'generated_text': 'No more of talk where god or angel guests need advice on how to take your family to the grocery store or buy an item before going to the store'},\n",
       " {'generated_text': 'No more of talk where god or angel guests are invited.\"\\n\\nWhat other things does this remind us about what the Bible says about men and other'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"No more of talk where god or angel guests\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4671a9c3",
   "metadata": {},
   "source": [
    "Probability attempt\n",
    "\n",
    "https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc89291d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_ids = tokenizer(\"Today is a nice day\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "generated_outputs = gpt2.generate(input_ids, num_beams=2, num_return_sequences=2, output_scores=True, length_penalty=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c68df72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BeamSearchDecoderOnlyOutput in module transformers.generation_utils object:\n",
      "\n",
      "class BeamSearchDecoderOnlyOutput(transformers.file_utils.ModelOutput)\n",
      " |  BeamSearchDecoderOnlyOutput(sequences: torch.LongTensor = None, sequences_scores: Union[torch.FloatTensor, NoneType] = None, scores: Union[Tuple[torch.FloatTensor], NoneType] = None, beam_indices: Union[Tuple[Tuple[torch.LongTensor]], NoneType] = None, attentions: Union[Tuple[Tuple[torch.FloatTensor]], NoneType] = None, hidden_states: Union[Tuple[Tuple[torch.FloatTensor]], NoneType] = None) -> None\n",
      " |  \n",
      " |  Base class for outputs of decoder-only generation models using beam search.\n",
      " |  \n",
      " |  Args:\n",
      " |      sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n",
      " |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      " |          if all batches finished early due to the `eos_token_id`.\n",
      " |      sequences_scores (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      " |          Final beam scores of the generated `sequences`.\n",
      " |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      " |          Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n",
      " |          of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.\n",
      " |          `(max_length-input_ids.shape[-1],)`-shaped tuple of `torch.FloatTensor` with each tensor of shape\n",
      " |          `(batch_size*num_beams*num_return_sequences, config.vocab_size)`).\n",
      " |      beam_indices (`tuple(tuple(torch.LongTensor))`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      " |          Beam indices of generated token id at each generation step. `(batch_size*num_return_sequences)`-shaped\n",
      " |          tuple of `(max_length-input_ids.shape[-1],)`-shaped tuples of scalar `torch.LongTensor` tensors.\n",
      " |      attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      " |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      " |          `torch.FloatTensor` of shape `(batch_size*num_beams, num_heads, generated_length, sequence_length)`.\n",
      " |      hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      " |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      " |          `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BeamSearchDecoderOnlyOutput\n",
      " |      transformers.file_utils.ModelOutput\n",
      " |      collections.OrderedDict\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |  \n",
      " |  __init__(self, sequences: torch.LongTensor = None, sequences_scores: Union[torch.FloatTensor, NoneType] = None, scores: Union[Tuple[torch.FloatTensor], NoneType] = None, beam_indices: Union[Tuple[Tuple[torch.LongTensor]], NoneType] = None, attentions: Union[Tuple[Tuple[torch.FloatTensor]], NoneType] = None, hidden_states: Union[Tuple[Tuple[torch.FloatTensor]], NoneType] = None) -> None\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'attentions': typing.Union[typing.Tuple[typing.Tupl...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'attentions': Field(name='attentions',type=typ...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  attentions = None\n",
      " |  \n",
      " |  beam_indices = None\n",
      " |  \n",
      " |  hidden_states = None\n",
      " |  \n",
      " |  scores = None\n",
      " |  \n",
      " |  sequences = None\n",
      " |  \n",
      " |  sequences_scores = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.file_utils.ModelOutput:\n",
      " |  \n",
      " |  __delitem__(self, *args, **kwargs)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __getitem__(self, k)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __post_init__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setitem__(self, key, value)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  pop(self, *args, **kwargs)\n",
      " |      od.pop(k[,d]) -> v, remove specified key and return the corresponding\n",
      " |      value.  If key is not found, d is returned if given, otherwise KeyError\n",
      " |      is raised.\n",
      " |  \n",
      " |  setdefault(self, *args, **kwargs)\n",
      " |      Insert key with a value of default if key is not in the dictionary.\n",
      " |      \n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  to_tuple(self) -> Tuple[Any]\n",
      " |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      " |  \n",
      " |  update(self, *args, **kwargs)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.OrderedDict:\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Return state information for pickling\n",
      " |  \n",
      " |  __reversed__(...)\n",
      " |      od.__reversed__() <==> reversed(od)\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      od.clear() -> None.  Remove all items from od.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      od.copy() -> a shallow copy of od\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  move_to_end(self, /, key, last=True)\n",
      " |      Move an existing element to the end (or beginning if last is false).\n",
      " |      \n",
      " |      Raise KeyError if the element does not exist.\n",
      " |  \n",
      " |  popitem(self, /, last=True)\n",
      " |      Remove and return a (key, value) pair from the dictionary.\n",
      " |      \n",
      " |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.OrderedDict:\n",
      " |  \n",
      " |  fromkeys(iterable, value=None) from builtins.type\n",
      " |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from collections.OrderedDict:\n",
      " |  \n",
      " |  __dict__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if the dictionary has the specified key, else False.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  get(self, key, default=None, /)\n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(generated_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13e2e67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_ids = tokenizer(\"Today is a nice day\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "generated_outputs = gpt2.generate(input_ids, do_sample=True, num_return_sequences=3, output_scores=True)\n",
    "\n",
    "# only use id's that were generated\n",
    "# gen_sequences has shape [3, 15]\n",
    "gen_sequences = generated_outputs.sequences[:, input_ids.shape[-1]:]\n",
    "\n",
    "# let's stack the logits generated at each step to a tensor and transform\n",
    "# logits to probs\n",
    "probs = torch.stack(generated_outputs.scores, dim=1).softmax(-1)  # -> shape [3, 15, vocab_size]\n",
    "\n",
    "# now we need to collect the probability of the generated token\n",
    "# we need to add a dummy dim in the end to make gather work\n",
    "gen_probs = torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1)\n",
    "\n",
    "# now we can do all kinds of things with the probs\n",
    "\n",
    "# 1) the probs that exactly those sequences are generated again\n",
    "# those are normally going to be very small\n",
    "unique_prob_per_sequence = gen_probs.prod(-1)\n",
    "\n",
    "# 2) normalize the probs over the three sequences\n",
    "normed_gen_probs = gen_probs / gen_probs.sum(0)\n",
    "assert normed_gen_probs[:, 0].sum() == 1.0, \"probs should be normalized\"\n",
    "\n",
    "# 3) compare normalized probs to each other like in 1)\n",
    "unique_normed_prob_per_sequence = normed_gen_probs.prod(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfbab4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8899e-13, 1.8545e-12, 8.7922e-11])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_normed_prob_per_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b279a685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today is a nice day!\" was all I could think to myself. However, this day\\'s victory',\n",
       " 'Today is a nice day for the American people to remember the brave soldiers that the country fought in WWII',\n",
       " \"Today is a nice day for a number of reasons: One, there's so many wonderful restaurants and\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated_outputs.sequences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:superpytorch]",
   "language": "python",
   "name": "conda-env-superpytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
